% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bootstrap.R
\name{mlboot}
\alias{mlboot}
\title{Calculate bootstrap confidence intervals for a performance metric}
\usage{
mlboot(
  .data,
  trusted,
  predicted,
  metric,
  cluster,
  pairwise = TRUE,
  n_boot = 2000,
  interval = 0.95,
  null = 0,
  ...
)
}
\arguments{
\item{.data}{Required. A dataframe containing trusted labels and predicted
labels where each row is a single object/observation and each column is a
variable describing that object/observation.}

\item{trusted}{Required. The name of a single variable in \code{.data} that
contains trusted labels.}

\item{predicted}{Required. A vector of names of one or more variables in
\code{.data} that contains predicted labels.}

\item{metric}{Required. A function that takes in at least two arguments (for
trusted labels and predicted labels, plus any additional customization
arguments) and returns a single number indicating performance. A number of
scoring/metric functions are built into the package and custom functions
can be developed as well.}

\item{cluster}{Optional. The name of a single variable in \code{.data} that
contains the cluster membership of each object/observation.}

\item{pairwise}{Optional. A logical indicating whether to estimate the
difference between all pairs of predicted labels (default = TRUE).}

\item{n_boot}{Optional. A positive integer indicating how many bootstrap
resamples the confidence intervals should be estimated from (default =
2000).}

\item{interval}{Optional. A number between 0 and 1 indicating the confidence
level of the confidence intervals to be estimated, such that 0.95 yields
95\% confidence intervals (default = 0.95).}

\item{null}{Optional. A single number to compare the bootstrap estimate to
when calculating p-values (default = 0).}

\item{...}{Optional. Additional arguments to pass along to the \code{metric}
function.}
}
\value{
A list containing the results and a description of the analysis.
\item{type}{A string indicating whether a single predictive model was
examined or two models were compared} \item{metric}{A string indicating the
name of the performance metric function used} \item{ntotal}{An integer
indicating the total number of examples in the test set} \item{ncluster}{An
integer indicating the number of clusters present in the test set}
\item{nboot}{An integer indicating the number of bootstrap resamples used
to estimate confidence intervals} \item{interval}{The confidence level of
the confidence intervals} \item{score_obs}{A vector containing the observed
performance score for the first model and, if applicable, the second model
and their difference} \item{score_cil}{A vector containing the lower bounds
of the confidence intervals corresponding to the observed performance
scores} \item{score_ciu}{A vector containing the upper bounds of the
confidence intervals corresponding to the observed performance scores}
\item{score_pval}{A vector containing p-values for the performance scores}
\item{resamples}{A matrix containing the performance scores and, if
applicable, their difference in each bootstrap resample}
}
\description{
Calculate the performance scores for one or two predictive models (and their
difference) on a given testing set using an arbitrary performance metric and
estimate bootstrap confidence intervals around these scores. Bootstrapping
can be customized to be basic nonparametric or cluster nonparameter, etc.
}
